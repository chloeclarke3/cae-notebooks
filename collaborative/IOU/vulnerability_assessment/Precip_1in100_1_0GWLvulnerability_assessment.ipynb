{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from climakitae.explore.vulnerability import cava_data\n",
    "from climakitae.explore.vulnerability_table import create_vul_table\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import logging\n",
    "import shutil\n",
    "import subprocess\n",
    "from multiprocessing import Pool\n",
    "\n",
    "instance_name = os.popen('curl -H \"Metadata-Flavor: Google\" http://metadata/computeMetadata/v1/instance/name').read()\n",
    "print(\"instance name:\",instance_name)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    _dir = os.getcwd()\n",
    "    _loc_files = glob.glob(_dir + '/*lab*.csv')\n",
    "print(_loc_files)\n",
    "\n",
    "num_processes = len(_loc_files) \n",
    "print('number of parallel processes:',num_processes)\n",
    "\n",
    "# set up location for output files to be moved\n",
    "BUCKET_NAME=\"analyticsengine\"\n",
    "OUTPUT_FOLDER=\"NetCDF_Output/1_0WL\"\n",
    "\n",
    "# Custom function to check for match based on pattern for file names\n",
    "def custom_match(search_string, file_name):\n",
    "    search_parts = search_string.split('*')\n",
    "    if all(part in file_name for part in search_parts):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "search_string = f'one_in_100_1_day_precipitation_1degreeWL_*.nc'\n",
    "result = subprocess.run([\"gsutil\", \"ls\", f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}/{search_string}\"], capture_output=True, text=True)\n",
    "\n",
    "if result.returncode == 0:\n",
    "    files = result.stdout.split('\\n')\n",
    "else:\n",
    "    print(\"Error running gsutil ls command:\", result.stderr)\n",
    "print('locations that have already been run:')\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_file(file, csv_file_name):\n",
    "    df = pd.read_csv(file)\n",
    "    \n",
    "    # create a log file\n",
    "    logging.basicConfig(filename=f'{instance_name.strip()}_{csv_file_name}_1.0GWL_precip_output.log', level=logging.INFO)\n",
    "\n",
    "    # Write to the log file\n",
    "    logging.info(f\"Processing {csv_file_name}\")\n",
    "        \n",
    "    for pt in range(len(df)):\n",
    "        # first check if this location and scenario has been run before \n",
    "        lat_str = str(df.loc[pt, 'lat']).replace('.', '')\n",
    "        lon_str = str(df.loc[pt, 'lon']).replace('.', '')\n",
    "        search_string = f'one_in_100_1_day_precipitation_1degreeWL_{lat_str}*_{lon_str}*.nc'\n",
    "        \n",
    "        # Write to the log file\n",
    "        logging.info(f\"checking if we need to run for SCE location {df.loc[pt, 'gridcode']}\")\n",
    "\n",
    "        # Check if file exists in the pre-fetched list\n",
    "        found_files = [file for file in files if custom_match(search_string, file)]\n",
    "        if found_files:\n",
    "            logging.info(f\"Already processes SCE location  {df.loc[pt, 'gridcode']}\")\n",
    "            logging.info(f\"Found files for '{search_string}':\")\n",
    "        else: # if the file does not exist run the cava_data retrevial method\n",
    "            logging.info(f\"Processing SCE location {df.loc[pt, 'gridcode']}\")\n",
    "        \n",
    "            data = cava_data(\n",
    "                ## Set-up\n",
    "                df[pt:pt+1],\n",
    "                downscaling_method=\"Statistical\",  # LOCA2 data \n",
    "                approach=\"Warming Level\",  \n",
    "                warming_level=1.0,\n",
    "\n",
    "                ## 1-in-X event specific arguments\n",
    "                variable=\"Precipitation (total)\",\n",
    "                metric_calc=\"max\", # daily maximum precipitation\n",
    "                one_in_x=100, # One-in-X\n",
    "                distr=\"gev\", # change distribution\n",
    "                units=\"inches\", # change units\n",
    "\n",
    "                ## Export\n",
    "                export_method=\"calculate\",\n",
    "                file_format=\"NetCDF\")\n",
    "\n",
    "            lat_str = str(df.loc[pt,'lat']).replace('.', '')\n",
    "            lon_str = str(df.loc[pt,'lon']).replace('.', '')\n",
    "            filename = glob.glob(f'one_in_100_1_day_precipitation_1degreeWL_{lat_str}*_{lon_str}*.nc')\n",
    "            filename = filename[0]\n",
    "\n",
    "            # Move the created .nc file to a different location\n",
    "            subprocess.run([\"gsutil\", \"mv\", filename, f\"gs://{BUCKET_NAME}/{OUTPUT_FOLDER}/{filename}\"])\n",
    "\n",
    "             # Write to the log file\n",
    "            logging.info(f\"Moved {filename} to gs://{BUCKET_NAME}/{OUTPUT_FOLDER}/{filename}\")\n",
    "\n",
    "# Create a pool of processes to parallelize the processing\n",
    "with Pool(processes=num_processes) as pool:\n",
    "    # Map the process_file function with arguments file and filename\n",
    "    pool.starmap(process_file, [(file, os.path.basename(file)) for file in _loc_files])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
