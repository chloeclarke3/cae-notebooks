{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90ef497a-dcb7-4b23-aa33-2b7690294144",
   "metadata": {},
   "source": [
    "# Vulnerability Assessment Pilot\n",
    "This notebook demonstrates on-going development of climate adaptation vulnerability assessment (CAVA) support using climate data in the Analytics Engine. \n",
    "\n",
    "To execute a given 'cell' of this notebook, place the cursor in the cell and press the 'play' icon, or simply press shift+enter together. Some cells will take longer to run, and you will see a [$\\ast$] to the left of the cell while AE is still working.\n",
    "\n",
    "**Intended Application**: As a user, I want to **<span style=\"color:#FF0000\">access climate projections data for my vulnerability assessment report</span>** by:\n",
    "1. Retrieve data metrics required for planning needs\n",
    "\n",
    "**Runtime**: With the default settings, this notebook takes approximately **several hours** to run from start to finish, depending on the metric choice. Modifications to selections may increase the runtime. \n",
    "\n",
    "### Step 0: Set-up\n",
    "\n",
    "First, we'll import the Python library [climakitae](https://github.com/cal-adapt/climakitae), our AE toolkit for climate data analysis, along with this specific functions from that library that we'll use in this notebook, as well as any other necessary Python libraries to aid in analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be3d6530-a94d-416e-81a3-2fac5e79bf72",
   "metadata": {
    "tags": [
     "cava_run"
    ]
   },
   "outputs": [],
   "source": [
    "from climakitae.explore.vulnerability import cava_data\n",
    "from climakitae.explore.vulnerability_table import create_vul_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153b50c3-5e8c-4f4c-b7e2-1839ae0c462f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Step 1: Import locations\n",
    "Now we'll read in point-based locations that we want to retrieve data for. For custom inputs, there are two options: (1) Input a single pair of latitude - longitude values; and (2) Import a csv file of locations that will each run. In the code below we show what each option looks like. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c435c50f-dec5-4440-9bd6-e5207a317f92",
   "metadata": {},
   "source": [
    "To import your own custom locations, we recommend putting your csv file in the same folder as this notebook for ease:\n",
    "1. Drag and drop a csv file into the file tree on the left hand side; or\n",
    "2. Use the `upload` button (the \"up arrow\" symbol next to the large blue plus symbol above the file tree). \n",
    "\n",
    "<span style=\"color:#FF0000\">**Formatting note**</span>: For the code cells below to work, there must be **2 columns labeled `lat` and `lon`**. Functionality to accept different labeling is forthcoming!\n",
    "\n",
    "In the cell below, we read the csv file in. We use the HadISD station list as an example here -- you may want to replace with your own locations file!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9617bc1-d883-4a14-9adf-47456abf1f82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "locations = pd.read_csv('*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bafbc5-2695-422e-a6b7-4795aeb418ad",
   "metadata": {},
   "source": [
    "### Step 2: Retrieve metric data\n",
    "\n",
    "The `cava_data` funciontality is designed to provide flexibility over customizable metric calculation. There are 4 customizable metrics that can be built with this functionality:\n",
    "1. Likely seasonal event occurence (e.g., \"likely summer night low temperature\")\n",
    "2. 1-in-X temperature events (e.g., \"1-in-10 year maximum temperature\")\n",
    "3. High/Extreme Heat Index events (e.g., \"how many days per year does the Heat Index exceed 90Â°F\")\n",
    "4. 1-in-X precipitation events (e.g., \"1-in-100 year, 24 hour precipitation\")\n",
    "\n",
    "Below is a table outlining all avaialble arguments to the `cava_data` function. The \"Required\" flag notes whether the argument must be passed to start generating data. Input options for each argument is provided, as well as whether a setting is required for any of the required selections. We provide multiple examples of working with the `cava_data` function with multiple configurations.\n",
    "\n",
    "| Argument | Options | Argument required for | Notes |\n",
    "|----------|---------|-----------------------|-------|\n",
    "|input_locations | Pass a location via csv. | All | Option to run either a single location, or multiple when **batch_mode=True**.|\n",
    "|variable | \"Air Temperature at 2m\", \"NOAA Heat Index\", \"Precipitation (total)\"| All | |\n",
    "|approach | \"Time\", \"Warming Level\" | All | |\n",
    "|downscaling_method | \"Dynamical\", \"Statistical\"| All | |\n",
    "|time_start_year | Numerical (min is 1981) | Required for **approach=Time** | |\n",
    "|time_end_year | Numerical (max is 2100) | Required for **approach=Time**| |\n",
    "|historical_data| \"Historical Climate\", \"Historical Reconstruction\" | Required for **approach=Time**| **Historical Climate** ranges from 1980-2015 for WRF and 1950-2015 for LOCA2-Hybrid. **Historical Reconstruction** ranges from 1950-2022. Historical Reconstruction data cannot be combined with SSP data.|\n",
    "|ssp_data | \"[SSP2-4.5]\", \"[SSP3-7.0]\", \"[SSP5-8.5]\" | Required for **approach=Time** | Dynamical only has SSP3-7.0, Statisical has all 3 SSP options|\n",
    "|warming_level| 1.5, 2.0, 2.5, 3.0 | Required for **approach=Warming Level**| |\n",
    "|metric_calc| \"max\", \"min\" | Required for 1-in-X events, Heat Index, likely seasonal event | |\n",
    "|heat_idx_threshold | Numerical | Required for Heat Index | Heat Index can only be calculated with **downscaling_method=\"Dynamical\"**|\n",
    "|one_in_x | Numerical | Required for 1-in-X events| |\n",
    "|event_duration| Numerical + \"day\"/\"hour\"| Optional for 1-in-X events| Must adhere to following structure: ({number}, \"{temporal frequency}\"). Temporal frequency options: \"day\", \"hour\". Default is (1, \"day\").|\n",
    "|distr| \"gev\", \"genpareto\", \"gumbel\", \"wibull\", \"pearson3\", \"gamma\"| Optional for 1-in-X events |Default set to \"gev\". |\n",
    "|percentile | Numerical (0-100) | Required for likely seasonal event |   |\n",
    "|season| \"summer\", \"winter\", \"all\" | Required for likely seasonal event| Default set to \"all\"|\n",
    "|units | Temp/Heat Index: \"degF\", \"degC\", \"K\". Precip: \"mm\", \"inches\" | Optional | Default for temp/Heat Index is DegF. Default for precip is mm.|\n",
    "|wrf_bias_adjust| True, False | Optional| Option to select only the 4 bias-adjusted WRF models. Only applicable for **downscaling_method=\"Dynamical\"**. Default set to True |\n",
    "|export_method| \"raw\", \"calculate\", \"both\" | Optional | Default set to \"both\" |\n",
    "|file_format | \"NetCDF\", \"csv\" | Optional | Default set to \"NetCDF\" |\n",
    "|separate_files | True, False | Optional | Option to export separate files if multiple points are passed. Default set to True|\n",
    "|batch_mode | True, False | Optional, but recommmended for multiple locations. | Option to efficiently run multiple points. Separate files for export is turned off in batch mode. Default set to False. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1718991-771d-4f53-873c-3a774644b772",
   "metadata": {},
   "source": [
    "The following cells illustrate several examples of how to retrieve and calculate various configurations of the `cava_data` function. Below is a list of the examples; and more are coming soon as more functionality is built in!\n",
    "1. Likely seasonal event, single location, time approach, all WRF data, with custom percentile\n",
    "2. Likely seasonal event, batch mode for multiple locations, time approach, all WRF data, with custom percentile\n",
    "3. 1-in-X temperature event, single location, bias-adjusted WRF data only, time approach, with custom event frequency and distribution\n",
    "4. Heat index event, single location, time approach, with custom threshold\n",
    "5. Likely seasonal event, single location, warming level approach, all WRF data, wtih custom percentile\n",
    "6. Likely seasonal event, single location, warming level approach, all LOCA2 data, with custom percentile\n",
    "7. Heat index event, batch mode for multiple locations, time approach, Historical Reconstruction data, with custom threshold\n",
    "8. Likely seasonal event, batch mode for multiple locations, warming levels approach, all LOCA2 data, with custom percentile\n",
    "9. 1-in-X precipitation event, single location, all LOCA2 data, time approach, with custom return period\n",
    "10. 1-in-X precipitation event, single location, all WRF data, time approach, with custom return period, event duration, and distribution\n",
    "11. Example of reading the calculated metric data via xarray for easy viewing within this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904e2ad-3fce-4afd-a482-de4e5fb99e20",
   "metadata": {},
   "source": [
    "\n",
    "# 2.0C Warming Scenario\n",
    "!chmod +x upload_nc_to_gcs.sh\n",
    "# Loop through each location in example_locs\n",
    "for pt in range(len(locations)):\n",
    "    data = cava_data(\n",
    "        ## Set-up\n",
    "        example_locs[pt:(pt+1)],\n",
    "        downscaling_method=\"Statistical\",  # LOCA2 data \n",
    "        approach=\"Warming Level\",  \n",
    "        warming_level=1.0,\n",
    "        \n",
    "        ## 1-in-X event specific arguments\n",
    "        variable=\"Precipitation (total)\",\n",
    "        metric_calc=\"max\", # daily maximum precipitation\n",
    "        one_in_x=100, # One-in-X\n",
    "        distr=\"gev\", # change distribution\n",
    "        units=\"inches\", # change units\n",
    "        \n",
    "        ## Export\n",
    "        export_method=\"calculate\",\n",
    "        file_format=\"NetCDF\"\n",
    "        #batch_mode=True # batch mode for GWL LOCA still in development\n",
    "    )\n",
    "    ## Add function to upload all nc files to GCP bucket if mannually initiate the batch run from here instead of from cava-orchestrator\n",
    "    !./upload_nc_to_gcs.sh\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a3f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2.0C Warming Scenario\n",
    "!chmod +x upload_nc_to_gcs_2WL.sh\n",
    "# Loop through each location in example_locs\n",
    "for pt in range(len(example_locs)):\n",
    "    data = cava_data(\n",
    "        ## Set-up\n",
    "        example_locs[pt:(pt+1)],\n",
    "        downscaling_method=\"Statistical\",  # LOCA2 data \n",
    "        approach=\"Warming Level\",  \n",
    "        warming_level=2.0,\n",
    "        \n",
    "        ## 1-in-X event specific arguments\n",
    "        variable=\"Precipitation (total)\",\n",
    "        metric_calc=\"max\", # daily maximum precipitation\n",
    "        one_in_x=100, # One-in-X\n",
    "        distr=\"gev\", # change distribution\n",
    "        units=\"inches\", # change units\n",
    "        \n",
    "        ## Export\n",
    "        export_method=\"calculate\",\n",
    "        file_format=\"NetCDF\"\n",
    "        #batch_mode=True # batch mode for GWL LOCA still in development\n",
    "    )\n",
    "    ## Add function to upload all nc files to GCP bucket if mannually initiate the batch run from here instead of from cava-orchestrator\n",
    "    !./upload_nc_to_gcs_2WL.sh\n",
    "\n",
    "    # 1.5C Warming Scenario\n",
    "!chmod +x upload_nc_to_gcs_15WL.sh\n",
    "for pt in range(len(wl15_locs)):\n",
    "    data = cava_data(\n",
    "        ## Set-up\n",
    "        wl15_locs[pt:(pt+1)],\n",
    "        downscaling_method=\"Statistical\",  # LOCA2 data \n",
    "        approach=\"Warming Level\",  \n",
    "        warming_level=1.0,\n",
    "        \n",
    "        ## 1-in-X event specific arguments\n",
    "        variable=\"Precipitation (total)\",\n",
    "        metric_calc=\"max\", # daily maximum precipitation\n",
    "        one_in_x=100, # One-in-X\n",
    "        distr=\"gev\", # change distribution\n",
    "        units=\"inches\", # change units\n",
    "        \n",
    "        ## Export\n",
    "        export_method=\"calculate\",\n",
    "        file_format=\"NetCDF\"\n",
    "        #batch_mode=True # batch mode for GWL LOCA still in development\n",
    "    )\n",
    "    ## Add function to upload all nc files to GCP bucket if mannually initiate the batch run from here instead of from cava-orchestrator\n",
    "\n",
    "    !./upload_nc_to_gcs_15WL.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d856628-522d-4375-926a-744fc9df86cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Example: Looking at the `cava_data` output\n",
    "It may be useful to look at the `cava_data` output within this notebook to assess the results and make any changes to the data request. After running the `cava_data` function, in a new cell you can type `data` to view the xarray data object. Depending on your export setting (\"raw\", \"calculate\", \"both\"), you can also view the data object in a more user-friendly xarray view. We provide the code to do so in the next cell -- select which option matches your `cava_data` run and the export option you would like to view! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0f7725-79f3-4cd2-9063-a6b6b81faf91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locagrids_lab20.csvdata # looking at the full xarray data object; will be a dictionary of data arrays!\n",
    "# data['calc_data'] # looking at just the calculated data metric\n",
    "# data['raw'] # looking at just the raw input data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befaee12-c451-4827-a783-8bf6ca9257bc",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e6b5b8-ce77-4202-8f8f-73a6b2debc10",
   "metadata": {},
   "source": [
    "### Appendix: Table Generation Sample Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c558433-b9c2-4b3a-b001-cce58c9067e0",
   "metadata": {},
   "source": [
    "Below, you'll find code that generates a table with different climate data metrics used in a CAVA Report. Feel free to run it and check it out! It is still very much in progress. **This will take 30+ min. to run.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c35d388-1989-4716-8c9f-d78f435c9bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "percentile = 50\n",
    "heat_idx_threshold = 80\n",
    "one_in_x = 10 # currently, only can do `one_in_x` for one value at a time\n",
    "df = create_vul_table(example_locs.iloc[[10]], percentile, heat_idx_threshold, one_in_x)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "analytics_notebook",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "3.9.13_(Analytics_Notebook) (Local)",
   "language": "python",
   "name": "analytics_notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
